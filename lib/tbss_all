#!/usr/bin/env python

# ===============================================================================
# tbss (2019) pipeline is written by-
#
# TASHRIF BILLAH
# Brigham and Women's Hospital/Harvard Medical School
# tbillah@bwh.harvard.edu
#
# ===============================================================================
# See details at https://github.com/pnlbwh/tbss
# Submit issues at https://github.com/pnlbwh/tbss/issues
# View LICENSE at https://github.com/pnlbwh/tbss/blob/master/LICENSE
# ===============================================================================

import argparse
from tbssUtil import FILEDIR, LIBDIR, isfile, isdir, pjoin, makeDirectory, check_call, \
    dirname, abspath, environ, copyfile, warn, basename, getcwd, chdir, listdir, RAISE
from conversion import loadExecutable, read_imgs_masks, read_cases
from loadFiles import read_imgs, write_time
from multiprocessing import Pool
from psutil import cpu_count
from datetime import datetime
from _version import __version__
from glob import glob
from loadFiles import checkDuplicity

fslDataDir= pjoin(environ['FSLDIR'], 'data')

def main():

    print(f'''\n\n# ===============================================================================
# tbss (2019) pipeline is written by-
# 
# TASHRIF BILLAH
# Brigham and Women's Hospital/Harvard Medical School
# tbillah@bwh.harvard.edu
# 
# ===============================================================================
# See details at https://github.com/pnlbwh/tbss
# Submit issues at https://github.com/pnlbwh/tbss/issues
# View LICENSE at https://github.com/pnlbwh/tbss/blob/master/LICENSE
# ===============================================================================
# __version__ {__version__}\n\n''')

    parser = argparse.ArgumentParser(
        description='TBSS at PNL encapsulating different protocols i.e FSL, ENIGMA, ANTs template etc.')

    parser.add_argument('--modality', type= str, default= 'FA', help= """Modality={FA,MD,AD,RD...} of images to run TBSS on

(i) single modality analysis:
you must run --modality FA first, then you can run for other modalities such as --modality AD

(ii) multi modality analysis:
first modality must be FA, and then the rest i.e --modality FA,MD,AD,RD,...
files from FA TBSS analysis are used in rest of the modalities""")


    parser.add_argument('-i', '--input', type=str, help="""(i) DWI images and masks:
a txt/csv file with dwi1,mask1\\ndwi2,mask2\\n... ; TBSS will start by creating FA, MD, AD, and RD;
additionally, use --generate flag

(ii) single modality analysis:
a directory with one particular Modality={FA,MD,AD,RD,...} images, or
a txt/csv file with ModImg1\\nModImg2\\n...
TBSS will be done for specified Modality

(iii) multi modality analysis: 
comma-separated multiple input directories corresponding to the sequence of --modality, or
a txt/csv file with Mod1_Img1,Mod2_Img1,...\\nMod1_Img2,Mod2_Img2,...\\n... ;
TBSS will be done for FA first, and then for other modalities.

(iv) separate nonFA TBSS:
if you wish to run TBSS for other modalities in future, files created during FA TBSS will be 
integrated into the nonFA TBSS. Provide the same --outDir as that of FA TBSS. All other directories will be 
realized relative to --outDir. On the other hand, provide --input and --modality in the same way 
as you would provide for FA TBSS.""")

    parser.add_argument('--generate', action='store_true',
                        help='generate diffusion measures for dwi1,mask1\\n... list')

    parser.add_argument('-c', '--caselist', type=str,
                        help='caselist.txt where each line is a subject ID')

    parser.add_argument('-o', '--outDir', type=str, required=True,
                        help='where all outputs are saved in an organized manner, '
                             'for separate/future nonFA TBSS--it must be the same as that of previous FA TBSS')

    parser.add_argument('--studyTemplate', action='store_true',
                        help='create all of template, templateMask, skeleton, skeletonMask, and skeletonMaskDst')

    parser.add_argument('--enigma', action='store_true',
                        help='use ENGIMA provided template, templateMask, skeleton, skeletonMask, and skeletonMaskDst, '
                             'do JHU white matter atlas based ROI analysis using ENIGMA look up table')

    parser.add_argument('--fmrib', action='store_true',
                        help='use FSL provided template, and skeleton')

    parser.add_argument('--template', type=str,
                        help='an FA image template (i.e ENIGMA, IIT), '
                             'if not specified, ANTs template will be created from provided images, '
                             'for ANTs template creation, you must provide FA images, '
                             'once ANTs template is created, you can run TBSS on non FA images using that template')

    parser.add_argument('--templateMask', type=str,
                        help='mask of the FA template, if not provided, one will be created')

    parser.add_argument('--skeleton', type=str,
                        help='skeleton of the FA template, if not provided, one will be created')

    parser.add_argument('--skeletonMask', type=str,
                        help='mask of the provided skeleton')

    parser.add_argument('--skeletonMaskDst', type=str,
                        help='skeleton mask distance map')

    parser.add_argument('-s', '--space', type=str,
                        help='you may register your template (including ANTs) to another standard space i.e MNI, '
                             'not recommended for a template that is already in MNI space (i.e ENIGMA, IIT)')

    parser.add_argument('-l', '--labelMap', type=str,
                        help='labelMap (atlas) in standard space (i.e any WhiteMatter atlas from ~/fsl/data/atlases/')

    parser.add_argument('--lut', type= str,
                         help='look up table for specified labelMap (atlas)')

    parser.add_argument('--qc', action='store_true', help='halt TBSS pipeline to let the user observe quality of registration')
    parser.add_argument('--avg', action='store_true', help='average Left/Right components of tracts in the atlas')
    parser.add_argument('--force', action='store_true', help='overwrite existing directory/file')

    parser.add_argument('--verbose', action='store_true', help='print everything to STDOUT')


    parser.add_argument('-n','--ncpu', type= int, default= 4,
                        help='number of processes/threads to use (-1 for all available, may slow down your system, default %(default)s)')

    parser.add_argument('--SKEL_THRESH', type=str, default='0.2',
                        help=f'threshold for masking skeleton and projecting FA image upon the skeleton, default %(default)s')

    parser.add_argument('--SEARCH_RULE_MASK', type=str, default=pjoin(fslDataDir, 'standard', 'LowerCingulum_1mm.nii.gz'),
                        help=f'search rule mask for nonFA TBSS, see "tbss_skeleton --help", default %(default)s')

    parser.add_argument('--status', action='store_true',
                        help='prints progress of TBSS pipeline so far')

    parser.add_argument('--noFillHole', action='store_true', help='do not fill holes inside the brain in diffusion measure images')

    parser.add_argument('--noAllSkeleton', action='store_true', help='do not merge skeletons')

    parser.add_argument('--noHtml', action='store_true', 
                        help='do not generate summary.html file containing screenshots of axial, lateral, and sagital views')

    diffusionMeasures = ['FA', 'MD', 'AD', 'RD']

    args = parser.parse_args()

    args.outDir = abspath(args.outDir)
    args.xfrmDir = pjoin(args.outDir, 'transform')
    args.logDir = pjoin(args.outDir, 'log')

    if args.status:
        if not args.outDir:
            raise ValueError('Design of this program has changed. Provide --outDir with --status to check progress')

        from progress import show_progress
        show_progress(args.outDir, args.verbose)
        exit()
    

    # if modality is not FA, check if 'transform' directory exists
    # warp and affine will be used from this directory to study non FA images
    modalities = args.modality.split(',')
    if modalities[0]=='FA':
        # we cannot overwrite outDir, because previous output can be there
        makeDirectory(args.outDir)

        # we cannot overwrite logDir, because previous commands can be there
        makeDirectory(args.logDir)

    else:
        if 'FA' in modalities:
            raise ValueError('--modality FA,... must come first')
        else:
            try:
                listdir(args.xfrmDir)
            except:
                raise FileNotFoundError(f'{args.xfrmDir}/ containing warps and affines from FA registration does not exist. '
                                         'Make sure to provide the same -o as that of FA TBSS or run FA TBSS first.')
        
        
        warn(f'Running separate nonFA TBSS, with same settings of previous FA TBSS')

        # using previous files, --force is invalid
        args.force = False

        # templates are obtained/specified in the branch block
        args.template = None
        args.templateMask = None
        args.skeleton = None
        args.skeletonMask = None
        args.skeletonMaskDst = None

        # obtain --caselist, --space, --labelMap, --lut, --avg, --SEARCH_RULE_MASK, --SKEL_THRESH from log
        # read the latest log
        # parse the above keys
        with open(pjoin(args.outDir, 'log', 'commands.txt')) as f:
            lastCommand= f.read().strip().split('\n')[-1]

        lastArgs= lastCommand.split()
        for i, entity in enumerate(lastArgs):
            if entity=='--caselist':
                args.caselist= lastArgs[i+1]
            elif entity=='--space':
                args.space= lastArgs[i+1]
            elif entity=='--labelMap':
                args.labelMap= lastArgs[i+1]
            elif entity=='--lut':
                args.lut= lastArgs[i+1]
            elif entity=='--SKEL_THRESH':
                args.SKEL_THRESH= lastArgs[i+1]
            elif entity=='--SEARCH_RULE_MASK':
                args.SEARCH_RULE_MASK= lastArgs[i+1]
            elif entity=='--avg':
                args.avg= True
            elif entity=='--noFillHole':
                args.noFillHole= True
            elif entity=='--noAllSkeleton':
                args.noAllSkeleton= True
            elif entity=='--noHtml':
                args.noHtml= True


    if args.ncpu==-1:
        args.ncpu= cpu_count()


    # check executables ====================================================================================
    apps = ['antsApplyTransforms',
            'antsRegistrationSyNQuick.sh',
            'antsMultivariateTemplateConstruction2.sh',
            'antsRegistration',
            'dtifit',
            'tbss_1_preproc',
            'tbss_skeleton',
            'distancemap']

    for exe in apps:
        loadExecutable(exe)
    print('All executables are found, program will begin now ...')


    if not environ['ANTSPATH']:
        raise EnvironmentError('ANTSPATH is not set')

    # argument sanity check ================================================================================
    if not args.template and args.templateMask:
        raise AttributeError('--templateMask is invalid w/o --template')

    if not args.skeleton and (args.skeletonMask or args.skeletonMaskDst):
        raise AttributeError('--skeletonMask and/or --skeletonMaskDst is invalid w/o --skeleton')

    if args.labelMap and not args.lut:
        raise AttributeError('--labelmap is invalid w/o --lut')

    if not args.caselist and not modalities[0]!='FA':
        raise AttributeError('--caselist is mandatory, see "List creation" section in TUTORIAL.md about how to create one')



    # take absolute path for everything
    if args.caselist:
        args.caselist = abspath(args.caselist)
    if args.template:
        args.template = abspath(args.template)
    if args.templateMask:
        args.templateMask = abspath(args.templateMask)
    if args.skeleton:
        args.skeleton = abspath(args.skeleton)
    if args.skeletonMask:
        args.skeletonMask = abspath(args.skeletonMask)
    if args.skeletonMaskDst:
        args.skeletonMaskDst = abspath(args.skeletonMaskDst)
    if args.space:
        args.space = abspath(args.space)
    if args.labelMap:
        args.labelMap = abspath(args.labelMap)
    if args.lut:
        args.lut = abspath(args.lut)
    if args.SEARCH_RULE_MASK:
        args.SEARCH_RULE_MASK = abspath(args.SEARCH_RULE_MASK)

    args.statsDir = pjoin(args.outDir, 'stats')
    makeDirectory(args.statsDir, args.force)


    if args.studyTemplate:

        if modalities[0] == 'FA':
            # clear all template arguments
            print('--study template specified, overriding any provided templates')
            args.template = None
            args.templateMask = None
            args.skeleton = None
            args.skeletonMask = None
            args.skeletonMaskDst = None
        else:
            args.template = pjoin(args.statsDir, 'template0.nii.gz')
            args.templateMask = pjoin(args.statsDir, 'mean_FA_mask.nii.gz')
            args.skeleton = pjoin(args.statsDir, 'mean_FA_skeleton.nii.gz')
            args.skeletonMask = pjoin(args.statsDir, 'mean_FA_skeleton_mask.nii.gz')
            args.skeletonMaskDst = pjoin(args.statsDir, 'mean_FA_skeleton_mask_dst.nii.gz')


    elif args.enigma:
        # use enigma http://enigma.ini.usc.edu/wp-content/uploads/2013/02/enigmaDTI.zip provided templates
        print('--enigma template specified, '
              'using http://enigma.ini.usc.edu/wp-content/uploads/2013/02/enigmaDTI.zip provided templates')
        enigmaDir= pjoin(LIBDIR, 'data', 'enigmaDTI')
        args.template = pjoin(enigmaDir, 'ENIGMA_DTI_FA.nii.gz')
        args.templateMask = pjoin(enigmaDir, 'ENIGMA_DTI_FA_mask.nii.gz')
        args.skeleton = pjoin(enigmaDir, 'ENIGMA_DTI_FA_skeleton.nii.gz')
        args.skeletonMask = pjoin(enigmaDir, 'ENIGMA_DTI_FA_skeleton_mask.nii.gz')
        args.skeletonMaskDst = pjoin(enigmaDir, 'ENIGMA_DTI_FA_skeleton_mask_dst.nii.gz')

        args.lut = pjoin(enigmaDir, 'ENIGMA_look_up_table.txt')
        args.labelMap = pjoin(fslDataDir, 'atlases', 'JHU', 'JHU-ICBM-labels-1mm.nii.gz')

        args.avg= True

    elif args.fmrib:
        # use FSL provided templates
        print('--fmrib template specified, using $FSLDIR/data/standard/FMRIB58_FA*.nii.gz templates')
        args.template= pjoin(fslDataDir, 'standard', 'FMRIB58_FA_1mm.nii.gz')
        args.skeleton= pjoin(fslDataDir, 'standard', 'FMRIB58_FA-skeleton_1mm.nii.gz')

        if modalities[0] != 'FA':
            # obtain templateMask, skeletonMask, skeletonMaskDst from stats/
            args.templateMask = pjoin(args.statsDir, 'mean_FA_mask.nii.gz')
            args.skeletonMask = pjoin(args.statsDir, 'mean_FA_skeleton_mask.nii.gz')
            args.skeletonMaskDst = pjoin(args.statsDir, 'mean_FA_skeleton_mask_dst.nii.gz')


    # copy templates to stat directory with a softlink of proper name
    CWD= getcwd()
    chdir(args.statsDir)
    files= listdir(args.statsDir)


    # soft link all the provided templates
    if args.template:
        fileBaseName= basename(args.template)
        if fileBaseName not in files:
            check_call(f'ln -s {args.template}', shell= True)

    if args.templateMask:
        fileBaseName= basename(args.templateMask)
        if fileBaseName not in files:
            check_call(f'ln -s {args.templateMask}', shell=True)

    if args.skeleton:
        fileBaseName= basename(args.skeleton)
        if fileBaseName not in files:
            check_call(f'ln -s {args.skeleton}', shell= True)

    if args.skeletonMask:
        fileBaseName= basename(args.skeletonMask)
        if fileBaseName not in files:
            check_call(f'ln -s {args.skeletonMask}', shell= True)

    if args.skeletonMaskDst:
        fileBaseName= basename(args.skeletonMaskDst)
        if fileBaseName not in files:
            check_call(f'ln -s {args.skeletonMaskDst}', shell= True)



    chdir(CWD)

    # organize images into different directories ===========================================================

    # outDir
    #    |
    # ------------------------------------------------------------------------------------------------------
    #    |           |             |                |        |       |                   |           |
    #    |           |             |                |        |       |                   |           |
    # transform   template        FA                MD       AD      RD                 log        stats
    #                              |       (same inner file structure as that of FA)
    #                              |
    #                 ----------------------------------------
    #                  |         |         |       |        |
    #                 preproc  origdata  warped  skeleton  roi
    #
    # copy all FA into FA directory
    # put all preprocessed data into preproc directory
    # keep all warp/affine in transform directory
    # output all warped images in warped directory
    # output all skeletons in skel directory
    # output ROI based analysis files in roi directory
    # save all ROI statistics, mean, and combined images


    # config file for logging purpose only
    with open(pjoin(args.logDir, 'config.ini'), 'w') as f:
        f.write('[DEFAULT]\n')
        f.write(f'N_CPU = {args.ncpu}\n')
        f.write('diffusionMeasures = {}\n'.format((',').join(diffusionMeasures)))
        f.write('verbose = {}\n'.format(1 if args.verbose else 0))
        f.write(f'outDir= {args.outDir}\n')
        f.write('modalities = {}\n'.format((',').join(modalities)))


    num_modalities= len(modalities)


    # log the argument-adjusted given command ==============================================================
    args_string = [abspath(__file__)]
    for key in args.__dict__.keys():
        if args.__dict__[key]:
            if type(args.__dict__[key])== type(True):
                args_string.append(f'--{key}')
            else:
                args_string.append(f'--{key} {args.__dict__[key]}')

    # append the commands to serve as history
    with open(pjoin(args.logDir, 'commands.txt'), 'a') as f:
        f.write(datetime.now().strftime('%c')+'\t'+(' ').join(args_string)+'\n')


    for modality in modalities:
        modDir = pjoin(args.outDir, f'{modality}')
        makeDirectory(modDir, args.force)


    cases= read_cases(args.caselist)
    # copy caselist into logDir
    copyfile(args.caselist, pjoin(args.logDir, 'caselist.txt'))


    # write start time
    write_time(pjoin(args.logDir, 'start_time.txt'), datetime.now())

    # calculate diffusion measures =========================================================================
    # when input is a dwi/mask list
    if args.generate:
        from generate_diffusion_measures import generate_diffusion_measures

        for modality in diffusionMeasures:
            measureDir = pjoin(args.outDir, f'{modality}')
            # the following generates a warning since dirs are already created in Ln:247-249
            makeDirectory(measureDir, args.force)

        dwImgs, masks= read_imgs_masks(abspath(args.input))
        warn('Assuming cases in --caselist correspond to images in --input, make sure they are ordered correctly')

        pool= Pool(args.ncpu)
        # need to generate diffusion measures first
        for dwImgPath, maskPath, caseId in zip(dwImgs, masks, cases):
            pool.apply_async(func= generate_diffusion_measures,
                             args= (dwImgPath, maskPath, caseId, args.outDir), error_callback= RAISE)


        pool.close()
        pool.join()


    # when input is a modality image list
    elif isfile(args.input):
        allModImgs = read_imgs(abspath(args.input), num_modalities)

        for i, modality in enumerate(modalities):
            modDir = pjoin(args.outDir, f'{modality}')
            warn('Assuming cases in --caselist correspond to images in --input, make sure they are ordered correctly')
            for c, imgPath in zip(cases,allModImgs[:,i]):
            # check if input directory is modDir, if not copy
                if dirname(imgPath) is not modDir:
                    check_call(f'cp {imgPath} {modDir}/{c}.nii.gz', shell= True)


    # when input are directories
    else:
        dirs= [abspath(p) for p in args.input.split(',')]

        if len(dirs)!=num_modalities:
            raise AttributeError('--modality and --input don\'t match')

        for dir,modality in zip(dirs,modalities):
            modDir = pjoin(args.outDir, f'{modality}')
            # check if input directory is modDir, if not copy
            if dir is not modDir:
                givenImgs= glob(pjoin(dir,'*.nii.gz'))

                checkDuplicity(givenImgs,cases)

                for c in cases:
                    found= False
                    for imgPath in givenImgs:
                        if c in imgPath:
                            check_call(f'cp {imgPath} {modDir}/{c}.nii.gz', shell= True)
                            found= True
                            break
                    if not found:
                        raise FileNotFoundError(f'Image corresponding to caseid {c} couldn\'t be found in {dir}\n'
                                                 'Either remove the caseid or make sure corresponding image is there')


    from tbss_single import process
    for modality in modalities:
        print('\n\nTBSS for modality ', modality)
        # first pass is FA images
        modDir = pjoin(args.outDir, f'{modality}')
        # change args.input
        args.input= modDir
        # change args.modality
        args.modality= modality

        args= process(args)


    # write final time
    write_time(pjoin(args.logDir, 'final_time.txt'), datetime.now())


if __name__=='__main__':
    main()

